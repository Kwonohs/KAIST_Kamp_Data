{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, models, layers, optimizers, regularizers\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "\n",
    "#판다스 데이터 불러오기\n",
    "df_fault = pd.read_csv('dataset/outlier_data.csv')\n",
    "df_nor = pd.read_csv('dataset/press_data_normal.csv')\n",
    "\n",
    "#데이터 copy\n",
    "normal = df_nor.copy()\n",
    "outlier = df_fault.copy()\n",
    "\n",
    "#정상데이터 시각화 + use_col 활요하여 독립변수 추출\n",
    "use_col = ['AI0_Vibration', 'AI1_Vibration', 'AI2_Current']\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# for i in range(len(use_col)):\n",
    "#     plt.subplot(3, 1, i+1)\n",
    "#     plt.title(use_col[i], fontsize=30)\n",
    "#     plt.plot(normal[use_col[i]])\n",
    "#     plt.xticks(size=20)\n",
    "#     plt.yticks(size=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "#이상데이터 시각화\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# for i in range(len(use_col)):\n",
    "#     plt.subplot(3, 1, i+1)\n",
    "#     plt.title(use_col[i], fontsize=30)\n",
    "#     plt.plot(outlier[use_col[i]])\n",
    "#     plt.xticks(size=20)\n",
    "#     plt.yticks(size=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "#apply 활용하여 함수 적용\n",
    "normal[use_col] = normal[use_col].apply(lambda x: abs(x))\n",
    "outlier[use_col] = outlier[use_col].apply(lambda x: abs(x))\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# for i in range(len(use_col)):\n",
    "#     plt.subplot(3, 1, i+1)\n",
    "#     plt.title(use_col[i], fontsize=30)\n",
    "#     plt.plot(normal[use_col[i]])\n",
    "#     plt.xticks(size=20)\n",
    "#     plt.yticks(size=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# #이상데이터 시각화\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# for i in range(len(use_col)):\n",
    "#     plt.subplot(3, 1, i+1)\n",
    "#     plt.title(use_col[i], fontsize=30)\n",
    "#     plt.plot(outlier[use_col[i]])\n",
    "#     plt.xticks(size=20)\n",
    "#     plt.yticks(size=20)\n",
    "\n",
    "# 입력데이터, 타겟데이터 분류.\n",
    "X_normal = normal[use_col]\n",
    "y_normal = normal['Equipment_state']\n",
    "X_anomaly = outlier[use_col]\n",
    "y_anomaly = outlier['Equipment_state']\n",
    "\n",
    "# 데이터 split\n",
    "X_train_normal = X_normal[:15000]\n",
    "y_train_normal = y_normal[:15000]\n",
    "X_test_normal = X_normal[15000:]\n",
    "y_test_normal = y_normal[15000:]\n",
    "X_test_anomaly = X_anomaly\n",
    "y_test_anomaly = y_anomaly\n",
    "\n",
    "# 입력변수 스케일링\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_normal)\n",
    "X_test_normal_scaled = scaler.transform(X_test_normal)\n",
    "X_test_anomaly_scaled = scaler.transform(X_test_anomaly)\n",
    "\n",
    "# 종속변수 list -> numpy배열 변경\n",
    "y_train_normal = np.array(y_train_normal)\n",
    "y_test_normal = np.array(y_test_normal)\n",
    "y_test_anomaly = np.array(y_test_anomaly)\n",
    "\n",
    "# sequence 초기화\n",
    "sequence = 20\n",
    "offset = 100\n",
    "# 미래데이터 예측을 위해서 offset 설정\n",
    "X_train, Y_train = [], []\n",
    "for index in range(len(X_train_scaled) - sequence - offset):\n",
    "    X_train.append(X_train_scaled[index: index + sequence])\n",
    "    Y_train.append(y_train_normal[index + sequence + offset])\n",
    "\n",
    "X_test_normal, Y_test_normal = [], []\n",
    "for index in range(len(X_test_normal_scaled) - sequence - offset):\n",
    "    X_test_normal.append(X_test_normal_scaled[index: index + sequence])\n",
    "    Y_test_normal.append(y_test_normal[index + sequence + offset])\n",
    "\n",
    "X_test_anomal, Y_test_anomal = [], []\n",
    "for index in range(len(X_test_anomaly_scaled) - sequence - offset):\n",
    "    X_test_anomal.append(X_test_anomaly_scaled[index: index + sequence])\n",
    "    Y_test_anomal.append(y_test_anomaly[index + sequence + offset])\n",
    "\n",
    "# 데이터 split\n",
    "X_test_normal, Y_test_normal = np.array(X_test_normal), np.array(Y_test_normal)\n",
    "X_test_anomal, Y_test_anomal = np.array(X_test_anomal), np.array(Y_test_anomal)\n",
    "\n",
    "X_valid_normal, Y_valid_normal = X_test_normal[:880, :, :], Y_test_normal[:880]\n",
    "X_test_normal, Y_test_normal = X_test_normal[880:, :, :], Y_test_normal[880:]\n",
    "X_valid_anomal, Y_valid_anomal = X_test_anomal[:300, :, :], Y_test_anomal[:300]\n",
    "X_test_anomal, Y_test_anomal = X_test_anomal[300:, :, :], Y_test_anomal[300:]\n",
    "\n",
    "# 데이터 concat개념으로 보면 이해가 됨.\n",
    "X_valid = np.vstack((X_valid_normal, X_valid_anomal))\n",
    "Y_valid = np.hstack((Y_valid_normal, Y_valid_anomal))\n",
    "\n",
    "X_test = np.vstack((X_test_normal, X_test_anomal))\n",
    "Y_test= np.hstack((Y_test_normal, Y_test_anomal))\n",
    "\n",
    "X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "X_valid, Y_valid = np.array(X_valid), np.array(Y_valid)\n",
    "X_test, Y_test = np.array(X_test), np.array(Y_test)\n",
    "\n",
    "print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n",
    "print('X_valid:', X_valid.shape, 'Y_valid:', Y_valid.shape)\n",
    "print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n",
    "\n",
    "X_valid_0 = X_valid[Y_valid==0]\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_lstm_autoencoder.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "def LSTM_AE(sequence, n_features):\n",
    "    lstm_ae = models.Sequential()\n",
    "\n",
    "    lstm_ae.add(layers.LSTM(64, input_shape=(sequence, n_features), return_sequences=True))\n",
    "    lstm_ae.add(layers.LSTM(32, return_sequences=False)) # \n",
    "    lstm_ae.add(layers.RepeatVector(sequence)) # 시퀀스 길이 복원\n",
    "\n",
    "    lstm_ae.add(layers.LSTM(32, return_sequences=True))\n",
    "    lstm_ae.add(layers.LSTM(64, return_sequences=True))\n",
    "    lstm_ae.add(layers.TimeDistributed(layers.Dense(n_features)))\n",
    "    return lstm_ae\n",
    "\n",
    "lstm_ae = LSTM_AE(20, 3)\n",
    "lstm_ae.summary()\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=20, verbose=1)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta = 0.00001, patience=50, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "lstm_ae.compile(loss='mse', optimizer=optimizers.Adam(0.001))\n",
    "\n",
    "history = lstm_ae.fit(X_train, X_train, epochs = 600, batch_size = 128, callbacks = [reduce_lr, es, checkpoint], validation_data = (X_valid_0, X_valid_0))\n",
    "\n",
    "# 학습 기록 저장 -> 나중에 시각화를 위해 사용\n",
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='valid loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch');plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "def flatten(X):\n",
    "    flattened = np.empty((X.shape[0], X.shape[2]))\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened[i] = X[i, X.shape[1] - 1, :]\n",
    "    return flattened\n",
    "    \n",
    "valid_x_predictions = lstm_ae.predict(X_valid)\n",
    "mse = np.mean(np.power(flatten(X_valid) - flatten(valid_x_predictions), 2), axis=1)\n",
    "\n",
    "precision, recall, threshold = metrics.precision_recall_curve(list(Y_valid), mse)\n",
    "index_cnt = [cnt for cnt, (p, r) in enumerate(zip(precision, recall)) if p == r][0]\n",
    "threshold_final = threshold[index_cnt]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Precision/Recall Curve for threshold\", fontsize = 15)\n",
    "plt.plot(threshold[threshold <= 0.2], precision[1:][threshold <= 0.2], label='Precision')\n",
    "plt.plot(threshold[threshold <= 0.2], recall[1:][threshold <= 0.2], label=\"Recall\")\n",
    "plt.plot(threshold_final, precision[index_cnt], 'o', color='r', label='Optimal threshold')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Precision/Recall\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"precision: \", precision[index_cnt], ', recall:', recall[index_cnt])\n",
    "print('threshold: ', threshold_final)\n",
    "\n",
    "test_x_predictions = lstm_ae.predict(X_test)\n",
    "mse = np.mean(np.power(flatten(X_test) - flatten(test_x_predictions), 2), axis = 1)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Reconstruction Error for both classes\", fontsize=15)\n",
    "plt.plot(np.where(Y_test==0)[0], mse[Y_test==0], marker='o', linestyle='', label = \"Normal\")\n",
    "plt.plot(np.where(Y_test==1)[0], mse[Y_test==1], marker='o', linestyle='', label = 'Anomaly')\n",
    "plt.axhline(threshold_final, 0, len(Y_test), color = 'r', linestyle='--', label='Threshold for Anomaly')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "plt.show()\n",
    "\n",
    "pred_y = [1 if e > threshold_final else 0 for e in mse]\n",
    "\n",
    "conf_matrix = metrics.confusion_matrix(list(Y_test), pred_y)\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(conf_matrix, xticklabels=[0, 1], yticklabels=[0, 1], annot=True, fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5482a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_lstm_autoencoder.h5', compile=False)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "with open('history.pkl', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "\n",
    "plt.plot(history['loss'], label='train loss')\n",
    "plt.plot(history['val_loss'], label='valid loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch');plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "def flatten(X):\n",
    "    flattened = np.empty((X.shape[0], X.shape[2]))\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened[i] = X[i, X.shape[1] - 1, :]\n",
    "    return flattened\n",
    "    \n",
    "valid_x_predictions = model.predict(X_valid)\n",
    "mse = np.mean(np.power(flatten(X_valid) - flatten(valid_x_predictions), 2), axis=1)\n",
    "\n",
    "precision, recall, threshold = metrics.precision_recall_curve(list(Y_valid), mse)\n",
    "index_cnt = [cnt for cnt, (p, r) in enumerate(zip(precision, recall)) if p == r][0]\n",
    "threshold_final = threshold[index_cnt]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Precision/Recall Curve for threshold\", fontsize = 15)\n",
    "plt.plot(threshold[threshold <= 0.2], precision[1:][threshold <= 0.2], label='Precision')\n",
    "plt.plot(threshold[threshold <= 0.2], recall[1:][threshold <= 0.2], label=\"Recall\")\n",
    "plt.plot(threshold_final, precision[index_cnt], 'o', color='r', label='Optimal threshold')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Precision/Recall\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"precision: \", precision[index_cnt], ', recall:', recall[index_cnt])\n",
    "print('threshold: ', threshold_final)\n",
    "\n",
    "test_x_predictions = model.predict(X_test)\n",
    "mse = np.mean(np.power(flatten(X_test) - flatten(test_x_predictions), 2), axis = 1)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Reconstruction Error for both classes\", fontsize=15)\n",
    "plt.plot(np.where(Y_test==0)[0], mse[Y_test==0], marker='o', linestyle='', label = \"Normal\")\n",
    "plt.plot(np.where(Y_test==1)[0], mse[Y_test==1], marker='o', linestyle='', label = 'Anomaly')\n",
    "plt.axhline(threshold_final, 0, len(Y_test), color = 'r', linestyle='--', label='Threshold for Anomaly')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "plt.show()\n",
    "\n",
    "pred_y = [1 if e > threshold_final else 0 for e in mse]\n",
    "\n",
    "conf_matrix = metrics.confusion_matrix(list(Y_test), pred_y)\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(conf_matrix, xticklabels=[0, 1], yticklabels=[0, 1], annot=True, fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "740765e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.003577603408024897)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n",
    "X_train.shape\n",
    "X_valid.shape\n",
    "\n",
    "pred = model.predict(np.empty((1, X_test.shape[1],X_test.shape[2])))\n",
    "threshold_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a3bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras import models, layers, optimizers\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# # --------------------------\n",
    "# # 1. 데이터 로드\n",
    "# # --------------------------\n",
    "# df_fault = pd.read_csv('dataset/outlier_data.csv')\n",
    "# df_nor = pd.read_csv('dataset/press_data_normal.csv')\n",
    "\n",
    "# use_col = ['AI0_Vibration', 'AI1_Vibration', 'AI2_Current']\n",
    "\n",
    "# # 절댓값 처리\n",
    "# df_nor[use_col] = df_nor[use_col].abs()\n",
    "# df_fault[use_col] = df_fault[use_col].abs()\n",
    "\n",
    "# # Feature / Label\n",
    "# X_normal = df_nor[use_col]\n",
    "# y_normal = df_nor['Equipment_state']\n",
    "# X_anomaly = df_fault[use_col]\n",
    "# y_anomaly = df_fault['Equipment_state']\n",
    "\n",
    "# # --------------------------\n",
    "# # 2. 데이터 합치기\n",
    "# # --------------------------\n",
    "# X_all = pd.concat([X_normal, X_anomaly], axis=0).reset_index(drop=True)\n",
    "# y_all = pd.concat([y_normal, y_anomaly], axis=0).reset_index(drop=True)\n",
    "\n",
    "# # --------------------------\n",
    "# # 3. Train/Validation/Test 분리\n",
    "# # --------------------------\n",
    "# X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "#     X_all, y_all, test_size=0.15, random_state=42, stratify=y_all\n",
    "# )\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     X_temp, y_temp, test_size=0.15/0.85, random_state=42, stratify=y_temp\n",
    "# )\n",
    "\n",
    "# # --------------------------\n",
    "# # 4. MinMaxScaler\n",
    "# # --------------------------\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_valid_scaled = scaler.transform(X_valid)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # --------------------------\n",
    "# # 5. 시퀀스 생성 함수\n",
    "# # --------------------------\n",
    "# def make_sequences(X, y, sequence=20, offset=100):\n",
    "#     X_seq, Y_seq = [], []\n",
    "#     max_index = len(X) - sequence - offset\n",
    "#     if max_index <= 0:\n",
    "#         return np.empty((0, sequence, X.shape[1])), np.empty((0,))\n",
    "#     for i in range(max_index):\n",
    "#         X_seq.append(X[i:i+sequence])\n",
    "#         Y_seq.append(y[i + sequence + offset])\n",
    "#     return np.array(X_seq), np.array(Y_seq)\n",
    "\n",
    "# sequence = 20\n",
    "# offset = 100\n",
    "\n",
    "# X_train_seq, Y_train_seq = make_sequences(X_train_scaled, np.array(y_train), sequence, offset)\n",
    "# X_valid_seq, Y_valid_seq = make_sequences(X_valid_scaled, np.array(y_valid), sequence, offset)\n",
    "# X_test_seq, Y_test_seq = make_sequences(X_test_scaled, np.array(y_test), sequence, offset)\n",
    "\n",
    "# # --------------------------\n",
    "# # 6. 데이터 shape 확인\n",
    "# # --------------------------\n",
    "# print('X_train:', X_train_seq.shape, 'Y_train:', Y_train_seq.shape)\n",
    "# print('X_valid:', X_valid_seq.shape, 'Y_valid:', Y_valid_seq.shape)\n",
    "# print('X_test:', X_test_seq.shape, 'Y_test:', Y_test_seq.shape)\n",
    "\n",
    "# # --------------------------\n",
    "# # 7. LSTM Autoencoder 정의\n",
    "# # --------------------------\n",
    "# def LSTM_AE(sequence, n_features):\n",
    "#     lstm_ae = models.Sequential()\n",
    "#     # 인코더\n",
    "#     lstm_ae.add(layers.LSTM(64, input_shape=(sequence, n_features), return_sequences=True))\n",
    "#     lstm_ae.add(layers.LSTM(32, return_sequences=False))\n",
    "#     lstm_ae.add(layers.RepeatVector(sequence))\n",
    "#     # 디코더\n",
    "#     lstm_ae.add(layers.LSTM(32, return_sequences=True))\n",
    "#     lstm_ae.add(layers.LSTM(64, return_sequences=True))\n",
    "#     lstm_ae.add(layers.TimeDistributed(layers.Dense(n_features)))\n",
    "#     return lstm_ae\n",
    "\n",
    "# lstm_ae = LSTM_AE(sequence, X_train_seq.shape[2])\n",
    "# lstm_ae.compile(loss='mse', optimizer=optimizers.Adam(0.001))\n",
    "\n",
    "# # --------------------------\n",
    "# # 8. Callback 정의\n",
    "# # --------------------------\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=20, verbose=1)\n",
    "# es = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=20, verbose=1,\n",
    "#                    mode='min', restore_best_weights=True)\n",
    "\n",
    "# # --------------------------\n",
    "# # 9. 학습\n",
    "# # --------------------------\n",
    "# # Autoencoder 학습: 입력=출력\n",
    "# history = lstm_ae.fit(\n",
    "#     X_train_seq, X_train_seq,\n",
    "#     epochs=500,\n",
    "#     batch_size=128,\n",
    "#     validation_data=(X_valid_seq, X_valid_seq),\n",
    "#     callbacks=[reduce_lr, es]\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
