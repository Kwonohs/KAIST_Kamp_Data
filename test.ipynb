{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, models, layers, optimizers, regularizers\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#판다스 데이터 불러오기\n",
    "df_fault = pd.read_csv('dataset/outlier_data.csv')\n",
    "df_nor = pd.read_csv('dataset/press_data_normal.csv')\n",
    "\n",
    "#데이터 copy\n",
    "normal = df_nor.copy()\n",
    "outlier = df_fault.copy()\n",
    "\n",
    "#정상데이터 시각화 + use_col 활요하여 독립변수 추출\n",
    "use_col = ['AI0_Vibration', 'AI1_Vibration', 'AI2_Current']\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# for i in range(len(use_col)):\n",
    "#     plt.subplot(3, 1, i+1)\n",
    "#     plt.title(use_col[i], fontsize=30)\n",
    "#     plt.plot(normal[use_col[i]])\n",
    "#     plt.xticks(size=20)\n",
    "#     plt.yticks(size=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "#이상데이터 시각화\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# for i in range(len(use_col)):\n",
    "#     plt.subplot(3, 1, i+1)\n",
    "#     plt.title(use_col[i], fontsize=30)\n",
    "#     plt.plot(outlier[use_col[i]])\n",
    "#     plt.xticks(size=20)\n",
    "#     plt.yticks(size=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "#apply 활용하여 함수 적용\n",
    "normal[use_col] = normal[use_col].apply(lambda x: abs(x))\n",
    "outlier[use_col] = outlier[use_col].apply(lambda x: abs(x))\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# for i in range(len(use_col)):\n",
    "#     plt.subplot(3, 1, i+1)\n",
    "#     plt.title(use_col[i], fontsize=30)\n",
    "#     plt.plot(normal[use_col[i]])\n",
    "#     plt.xticks(size=20)\n",
    "#     plt.yticks(size=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# #이상데이터 시각화\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# for i in range(len(use_col)):\n",
    "#     plt.subplot(3, 1, i+1)\n",
    "#     plt.title(use_col[i], fontsize=30)\n",
    "#     plt.plot(outlier[use_col[i]])\n",
    "#     plt.xticks(size=20)\n",
    "#     plt.yticks(size=20)\n",
    "\n",
    "# 입력데이터, 타겟데이터 분류.\n",
    "X_normal = normal[use_col]\n",
    "y_normal = normal['Equipment_state']\n",
    "X_anomaly = outlier[use_col]\n",
    "y_anomaly = outlier['Equipment_state']\n",
    "\n",
    "# 데이터 split\n",
    "X_train_normal = X_normal[:15000]\n",
    "y_train_normal = y_normal[:15000]\n",
    "X_test_normal = X_normal[15000:]\n",
    "y_test_normal = y_normal[15000:]\n",
    "X_test_anomaly = X_anomaly\n",
    "y_test_anomaly = y_anomaly\n",
    "\n",
    "# 입력변수 스케일링\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_normal)\n",
    "X_test_normal_scaled = scaler.transform(X_test_normal)\n",
    "X_test_anomaly_scaled = scaler.transform(X_test_anomaly)\n",
    "\n",
    "# 종속변수 list -> numpy배열 변경\n",
    "y_train_normal = np.array(y_train_normal)\n",
    "y_test_normal = np.array(y_test_normal)\n",
    "y_test_anomaly = np.array(y_test_anomaly)\n",
    "\n",
    "# sequence 초기화\n",
    "sequence = 20\n",
    "\n",
    "# 미래데이터 예측을 위해서 offset 설정\n",
    "X_train, Y_train = [], []\n",
    "for index in range(len(X_train_scaled) - sequence - 100):\n",
    "    X_train.append(X_train_scaled[index: index + sequence])\n",
    "    Y_train.append(y_train_normal[index + sequence + 100])\n",
    "\n",
    "\n",
    "X_test_normal, Y_test_normal = [], []\n",
    "for index in range(len(X_test_normal_scaled) - sequence - 100):\n",
    "    X_test_normal.append(X_test_normal_scaled[index: index + sequence])\n",
    "    Y_test_normal.append(y_test_normal[index + sequence + 100])\n",
    "\n",
    "X_test_anomal, Y_test_anomal = [], []\n",
    "for index in range(len(X_test_anomaly_scaled) - sequence - 100):\n",
    "    X_test_anomal.append(X_test_anomaly_scaled[index: index + sequence])\n",
    "    Y_test_anomal.append(y_test_anomaly[index + sequence + 100])\n",
    "\n",
    "# 데이터 split\n",
    "X_test_normal, Y_test_normal = np.array(X_test_normal), np.array(Y_test_normal)\n",
    "X_test_anomal, Y_test_anomal = np.array(X_test_anomal), np.array(Y_test_anomal)\n",
    "\n",
    "X_valid_normal, Y_valid_normal = X_test_normal[:880, :, :], Y_test_normal[:880]\n",
    "X_test_normal, Y_test_normal = X_test_normal[880:, :, :], Y_test_normal[880:]\n",
    "X_valid_anomal, Y_valid_anomal = X_test_anomal[:300, :, :], Y_test_anomal[:300]\n",
    "X_test_anomal, Y_test_anomal = X_test_anomal[300:, :, :], Y_test_anomal[300:]\n",
    "\n",
    "# 데이터 concat개념으로 보면 이해가 됨.\n",
    "X_valid = np.vstack((X_valid_normal, X_valid_anomal))\n",
    "Y_valid = np.hstack((Y_valid_normal, Y_valid_anomal))\n",
    "\n",
    "X_test = np.vstack((X_test_normal, X_test_anomal))\n",
    "Y_test= np.hstack((Y_test_normal, Y_test_anomal))\n",
    "\n",
    "X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "X_valid, Y_valid = np.array(X_valid), np.array(Y_valid)\n",
    "X_test, Y_test = np.array(X_test), np.array(Y_test)\n",
    "\n",
    "print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n",
    "print('X_valid:', X_valid.shape, 'Y_valid:', Y_valid.shape)\n",
    "print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n",
    "\n",
    "X_valid_0 = X_valid[Y_valid==0]\n",
    "\n",
    "def LSTM_AE(sequence, n_features):\n",
    "    lstm_ae = models.Sequential()\n",
    "\n",
    "    lstm_ae.add(layers.LSTM(64, input_shape=(sequence, n_features), return_sequences=True))\n",
    "    lstm_ae.add(layers.LSTM(32, return_sequences=False))\n",
    "    lstm_ae.add(layers.RepeatVector(sequence))\n",
    "\n",
    "    lstm_ae.add(layers.LSTM(32, return_sequences=True))\n",
    "    lstm_ae.add(layers.LSTM(64, return_sequences=True))\n",
    "    lstm_ae.add(layers.TimeDistributed(layers.Dense(n_features)))\n",
    "    return lstm_ae\n",
    "\n",
    "lstm_ae = LSTM_AE(20, 3)\n",
    "lstm_ae.summary()\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=50, verbose=1)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta = 0.00001, patience=120, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "lstm_ae.compile(loss='mse', optimizer=optimizers.Adam(0.001))\n",
    "\n",
    "history = lstm_ae.fit(X_train, X_train, epochs = 800, batch_size = 128, callbacks = [reduce_lr, es], validation_data = (X_valid_0, X_valid_0))\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='valid loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch');plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "def flatten(X):\n",
    "    flattened = np.empty((X.shape[0], X.shape[2]))\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened[i] = X[i, X.shape[1] - 1, :]\n",
    "    return flattened\n",
    "    \n",
    "valid_x_predictions = lstm_ae.predict(X_valid)\n",
    "mse = np.mean(np.power(flatten(X_valid) - flatten(valid_x_predictions), 2), axis=1)\n",
    "\n",
    "precision, recall, threshold = metrics.precision_recall_curve(list(Y_valid), mse)\n",
    "index_cnt = [cnt for cnt, (p, r) in enumerate(zip(precision, recall)) if p == r][0]\n",
    "threshold_final = threshold[index_cnt]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Precision/Recall Curve for threshold\", fontsize = 15)\n",
    "plt.plot(threshold[threshold <= 0.2], precision[1:][threshold <= 0.2], label='Precision')\n",
    "plt.plot(threshold[threshold <= 0.2], recall[1:][threshold <= 0.2], label=\"Recall\")\n",
    "plt.plot(threshold_final, precision[index_cnt], 'o', color='r', label='Optimal threshold')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Precision/Recall\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"precision: \", precision[index_cnt], ', recall:', recall[index_cnt])\n",
    "print('threshold: ', threshold_final)\n",
    "\n",
    "test_x_predictions = lstm_ae.predict(X_test)\n",
    "mse = np.mean(np.power(flatten(X_test) - flatten(test_x_predictions), 2), axis = 1)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Reconstruction Error for both classes\", fontsize=15)\n",
    "plt.plot(np.where(Y_test==0)[0], mse[Y_test==0], marker='o', linestyle='', label = \"Normal\")\n",
    "plt.plot(np.where(Y_test==1)[0], mse[Y_test==1], marker='o', linestyle='', label = 'Anomaly')\n",
    "plt.axhline(threshold_final, 0, len(Y_test), color = 'r', linestyle='--', label='Threshold for Anomaly')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "plt.show()\n",
    "\n",
    "pred_y = [1 if e > threshold_final else 0 for e in mse]\n",
    "\n",
    "conf_matrix = metrics.confusion_matrix(list(Y_test), pred_y)\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(conf_matrix, xticklabels=[0, 1], yticklabels=[0, 1], annot=True, fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688304e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5482a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (14300, 20, 3) Y_train: (14300,)\n",
      "X_valid: (2970, 20, 3) Y_valid: (2970,)\n",
      "X_test: (2970, 20, 3) Y_test: (2970,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mymeta_corp/.pyenv/versions/tf_gpu_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - loss: 0.0126 - val_loss: 0.0108 - learning_rate: 0.0010\n",
      "Epoch 2/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0107 - val_loss: 0.0104 - learning_rate: 0.0010\n",
      "Epoch 3/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0104 - val_loss: 0.0101 - learning_rate: 0.0010\n",
      "Epoch 4/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0102 - val_loss: 0.0099 - learning_rate: 0.0010\n",
      "Epoch 5/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0101 - val_loss: 0.0099 - learning_rate: 0.0010\n",
      "Epoch 6/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0099 - val_loss: 0.0096 - learning_rate: 0.0010\n",
      "Epoch 7/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0097 - val_loss: 0.0096 - learning_rate: 0.0010\n",
      "Epoch 8/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0097 - val_loss: 0.0095 - learning_rate: 0.0010\n",
      "Epoch 9/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0097 - val_loss: 0.0095 - learning_rate: 0.0010\n",
      "Epoch 10/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0096 - val_loss: 0.0094 - learning_rate: 0.0010\n",
      "Epoch 11/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0095 - val_loss: 0.0093 - learning_rate: 0.0010\n",
      "Epoch 12/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0093 - val_loss: 0.0091 - learning_rate: 0.0010\n",
      "Epoch 13/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0092 - val_loss: 0.0090 - learning_rate: 0.0010\n",
      "Epoch 14/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0092 - val_loss: 0.0090 - learning_rate: 0.0010\n",
      "Epoch 15/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0091 - val_loss: 0.0090 - learning_rate: 0.0010\n",
      "Epoch 16/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0091 - val_loss: 0.0089 - learning_rate: 0.0010\n",
      "Epoch 17/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0091 - val_loss: 0.0089 - learning_rate: 0.0010\n",
      "Epoch 18/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - loss: 0.0090 - val_loss: 0.0088 - learning_rate: 0.0010\n",
      "Epoch 19/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0091 - val_loss: 0.0087 - learning_rate: 0.0010\n",
      "Epoch 20/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0088 - val_loss: 0.0086 - learning_rate: 0.0010\n",
      "Epoch 21/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0088 - val_loss: 0.0086 - learning_rate: 0.0010\n",
      "Epoch 22/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0087 - val_loss: 0.0086 - learning_rate: 0.0010\n",
      "Epoch 23/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0087 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 24/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0086 - val_loss: 0.0084 - learning_rate: 0.0010\n",
      "Epoch 25/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0085 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 26/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0084 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 27/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0083 - val_loss: 0.0081 - learning_rate: 0.0010\n",
      "Epoch 28/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0082 - val_loss: 0.0080 - learning_rate: 0.0010\n",
      "Epoch 29/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0081 - val_loss: 0.0079 - learning_rate: 0.0010\n",
      "Epoch 30/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0080 - val_loss: 0.0078 - learning_rate: 0.0010\n",
      "Epoch 31/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0079 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 32/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0078 - val_loss: 0.0076 - learning_rate: 0.0010\n",
      "Epoch 33/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0078 - val_loss: 0.0076 - learning_rate: 0.0010\n",
      "Epoch 34/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0077 - val_loss: 0.0076 - learning_rate: 0.0010\n",
      "Epoch 35/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0076 - val_loss: 0.0074 - learning_rate: 0.0010\n",
      "Epoch 36/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0076 - val_loss: 0.0073 - learning_rate: 0.0010\n",
      "Epoch 37/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0075 - val_loss: 0.0073 - learning_rate: 0.0010\n",
      "Epoch 38/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.0074 - val_loss: 0.0074 - learning_rate: 0.0010\n",
      "Epoch 39/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0074 - val_loss: 0.0072 - learning_rate: 0.0010\n",
      "Epoch 40/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0073 - val_loss: 0.0072 - learning_rate: 0.0010\n",
      "Epoch 41/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0073 - val_loss: 0.0071 - learning_rate: 0.0010\n",
      "Epoch 42/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0072 - val_loss: 0.0071 - learning_rate: 0.0010\n",
      "Epoch 43/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0072 - val_loss: 0.0071 - learning_rate: 0.0010\n",
      "Epoch 44/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0071 - val_loss: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 45/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0070 - val_loss: 0.0068 - learning_rate: 0.0010\n",
      "Epoch 46/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0069 - val_loss: 0.0068 - learning_rate: 0.0010\n",
      "Epoch 47/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0068 - val_loss: 0.0067 - learning_rate: 0.0010\n",
      "Epoch 48/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0068 - val_loss: 0.0066 - learning_rate: 0.0010\n",
      "Epoch 49/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0067 - val_loss: 0.0066 - learning_rate: 0.0010\n",
      "Epoch 50/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0067 - val_loss: 0.0066 - learning_rate: 0.0010\n",
      "Epoch 51/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0067 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 52/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0066 - val_loss: 0.0064 - learning_rate: 0.0010\n",
      "Epoch 53/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0065 - val_loss: 0.0063 - learning_rate: 0.0010\n",
      "Epoch 54/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0064 - val_loss: 0.0063 - learning_rate: 0.0010\n",
      "Epoch 55/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0063 - val_loss: 0.0062 - learning_rate: 0.0010\n",
      "Epoch 56/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0062 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 57/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0062 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 58/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0061 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 59/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0060 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 60/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.0060 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 61/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.0059 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 62/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0059 - val_loss: 0.0057 - learning_rate: 0.0010\n",
      "Epoch 63/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0058 - val_loss: 0.0056 - learning_rate: 0.0010\n",
      "Epoch 64/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0057 - val_loss: 0.0057 - learning_rate: 0.0010\n",
      "Epoch 65/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0057 - val_loss: 0.0057 - learning_rate: 0.0010\n",
      "Epoch 66/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0056 - val_loss: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 67/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0055 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 68/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0055 - val_loss: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 69/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0055 - val_loss: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 70/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0054 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 71/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0054 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 72/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0054 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 73/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0053 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 74/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0053 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 75/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0053 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 76/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0052 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 77/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0051 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 78/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0051 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 79/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0051 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 80/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0050 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 81/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0050 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 82/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0049 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 83/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0049 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 84/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0048 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 85/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0048 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 86/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0048 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 87/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0048 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 88/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0047 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 89/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0047 - val_loss: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 90/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0046 - val_loss: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 91/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0046 - val_loss: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 92/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0046 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 93/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0045 - val_loss: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 94/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0045 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 95/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - loss: 0.0045 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 96/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - loss: 0.0044 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 97/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0044 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 98/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0044 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 99/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0044 - val_loss: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 100/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0043 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 101/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0043 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 102/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0043 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 103/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0043 - val_loss: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 104/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0042 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 105/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 72ms/step - loss: 0.0042 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 106/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0042 - val_loss: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 107/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 92ms/step - loss: 0.0042 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 108/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 116ms/step - loss: 0.0041 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 109/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 106ms/step - loss: 0.0041 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 110/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 81ms/step - loss: 0.0041 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 111/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - loss: 0.0041 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 112/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 72ms/step - loss: 0.0040 - val_loss: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 113/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 72ms/step - loss: 0.0040 - val_loss: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 114/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0040 - val_loss: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 115/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0040 - val_loss: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 116/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 149ms/step - loss: 0.0039 - val_loss: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 117/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 125ms/step - loss: 0.0039 - val_loss: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 118/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0039 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 119/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 134ms/step - loss: 0.0039 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 120/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 120ms/step - loss: 0.0039 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 121/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 86ms/step - loss: 0.0039 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 122/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0038 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 123/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0038 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 124/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 125/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 126/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 127/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 128/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 129/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 130/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 154ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 131/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 141ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 132/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 100ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 133/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 80ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 134/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 91ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 135/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 136/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - loss: 0.0036 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 137/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0036 - val_loss: 0.0037 - learning_rate: 0.0010\n",
      "Epoch 138/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0036 - val_loss: 0.0036 - learning_rate: 0.0010\n",
      "Epoch 139/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - loss: 0.0035 - val_loss: 0.0036 - learning_rate: 0.0010\n",
      "Epoch 140/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - loss: 0.0035 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 141/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 124ms/step - loss: 0.0035 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 142/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 86ms/step - loss: 0.0035 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 143/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - loss: 0.0034 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 144/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0034 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 145/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0034 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 146/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0034 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 147/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0034 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 148/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0033 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 149/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - loss: 0.0033 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 150/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - loss: 0.0033 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 151/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - loss: 0.0033 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 152/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0033 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 153/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 110ms/step - loss: 0.0032 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 154/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 153ms/step - loss: 0.0032 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 155/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - loss: 0.0032 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 156/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - loss: 0.0032 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 157/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - loss: 0.0032 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 158/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 93ms/step - loss: 0.0032 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 159/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - loss: 0.0032 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 160/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - loss: 0.0032 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 161/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - loss: 0.0032 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 162/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0031 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 163/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 82ms/step - loss: 0.0031 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 164/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0031 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 165/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 85ms/step - loss: 0.0031 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 166/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0031 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 167/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - loss: 0.0030 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 168/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - loss: 0.0030 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 169/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - loss: 0.0030 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 170/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0030 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 171/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0030 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 172/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - loss: 0.0030 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 173/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0029 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 174/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0029 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 175/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0029 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 176/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0029 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 177/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0029 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 178/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0028 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 179/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0028 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 180/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0028 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 181/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 89ms/step - loss: 0.0028 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 182/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - loss: 0.0028 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 183/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - loss: 0.0028 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 184/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - loss: 0.0028 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 185/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0027 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 186/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0027 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 187/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0027 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 188/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0027 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 189/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0027 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 190/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - loss: 0.0027 - val_loss: 0.0028 - learning_rate: 0.0010\n",
      "Epoch 191/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0027 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 192/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0026 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 193/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0026 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 194/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0026 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 195/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - loss: 0.0026 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 196/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - loss: 0.0026 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 197/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0026 - val_loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 198/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0026 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 199/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0025 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 200/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - loss: 0.0025 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 201/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 91ms/step - loss: 0.0025 - val_loss: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 202/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0025 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 203/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0025 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 204/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - loss: 0.0024 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 205/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0024 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 206/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0024 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 207/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0024 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 208/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0024 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 209/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0024 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 210/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0024 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 211/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0023 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 212/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - loss: 0.0023 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 213/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 92ms/step - loss: 0.0023 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 214/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0023 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 215/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0023 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 216/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0023 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 217/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0023 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 218/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 82ms/step - loss: 0.0023 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 219/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 220/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 221/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 222/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 86ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 223/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 224/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 225/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 226/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 86ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 227/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 228/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0022 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 229/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 230/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0022 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 231/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0022 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 232/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 233/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0021 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 234/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 235/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 236/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 237/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 238/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 239/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 240/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 241/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 242/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 243/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 244/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0021 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 245/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 246/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0021 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 247/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0021 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 248/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0020 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 249/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0021 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 250/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 251/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 252/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 253/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 254/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 72ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 255/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 256/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 257/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0020 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 258/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 259/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 260/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0020 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 261/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0020 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 262/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 263/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0020 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 264/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0019 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 265/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0019 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 266/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0019 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 267/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0019 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 268/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 78ms/step - loss: 0.0019 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 269/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0019 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 270/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0019 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 271/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0019 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 272/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0019 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 273/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0019 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 274/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0019 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 275/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0018 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 276/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 277/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 118ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 278/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 183ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 279/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 179ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 280/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 98ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 281/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 111ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 282/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 170ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 283/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 217ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 284/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 75ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 285/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 85ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 286/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 287/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 288/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 289/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 290/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 291/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 292/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 293/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0018 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 294/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 295/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0017 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 296/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0017 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 297/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0017 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 298/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0017 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 299/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0017 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 300/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0017 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 301/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0017 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 302/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0017 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 303/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0017 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 304/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0017 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 305/500\n",
      "\u001b[1m111/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0017\n",
      "Epoch 305: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0017 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 306/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0017 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 307/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0017 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 308/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0017 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 309/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0017 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 310/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0017 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 311/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0017 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 312/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0017 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 313/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 314/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 315/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 316/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 317/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 318/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 319/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 320/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 321/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 322/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 323/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 324/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 325/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 326/500\n",
      "\u001b[1m111/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0016\n",
      "Epoch 326: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0016 - val_loss: 0.0017 - learning_rate: 7.0000e-04\n",
      "Epoch 327/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 328/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 329/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 330/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 331/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 332/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 333/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 334/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 335/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 336/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 337/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 338/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 339/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 340/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 341/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 342/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 343/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 344/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 345/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 346/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 347/500\n",
      "\u001b[1m111/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0015\n",
      "Epoch 347: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 4.9000e-04\n",
      "Epoch 348/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0015 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 349/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.0015 - val_loss: 0.0016 - learning_rate: 3.4300e-04\n",
      "Epoch 350/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0015 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 351/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0015 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 352/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0015 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 353/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0015 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 354/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0015 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 355/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 152ms/step - loss: 0.0015 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 356/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 74ms/step - loss: 0.0015 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 357/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 358/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 359/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 360/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 361/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 362/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 363/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 364/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 365/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 366/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 367/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 368/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 369/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 370/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 371/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 372/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 373/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 374/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 375/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 376/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 377/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 378/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 379/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 380/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 381/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 76ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 382/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 383/500\n",
      "\u001b[1m111/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0014\n",
      "Epoch 383: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 3.4300e-04\n",
      "Epoch 384/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 385/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 386/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 387/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 388/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 389/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 390/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 391/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 392/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 393/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 394/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 395/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 396/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 397/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 398/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 399/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 400/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 71ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 401/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 402/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 403/500\n",
      "\u001b[1m111/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0014\n",
      "Epoch 403: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 2.4010e-04\n",
      "Epoch 404/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 1.6807e-04\n",
      "Epoch 405/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 1.6807e-04\n",
      "Epoch 406/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 1.6807e-04\n",
      "Epoch 407/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 1.6807e-04\n",
      "Epoch 408/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 409/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 1.6807e-04\n",
      "Epoch 410/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 411/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 412/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 413/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 97ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 414/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 415/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 416/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 417/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 418/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 419/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 420/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0014 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 421/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 95ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 422/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 76ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 423/500\n",
      "\u001b[1m111/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0013\n",
      "Epoch 423: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.6807e-04\n",
      "Epoch 424/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 425/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 426/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 77ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 427/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 428/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 429/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 430/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 431/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 432/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 433/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 434/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 435/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 436/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 437/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 438/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 85ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 439/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 440/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 441/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 442/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 443/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 444/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 445/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 446/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 447/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 448/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 449/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 450/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 451/500\n",
      "\u001b[1m111/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0013\n",
      "Epoch 451: ReduceLROnPlateau reducing learning rate to 8.235429777414538e-05.\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 1.1765e-04\n",
      "Epoch 452/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 129ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 453/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 248ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 454/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 162ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 455/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 456/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 457/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 458/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 459/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 460/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 461/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 462/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 77ms/step - loss: 0.0013 - val_loss: 0.0014 - learning_rate: 8.2354e-05\n",
      "Epoch 463/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 77ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 8.2354e-05\n",
      "Epoch 464/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 8.2354e-05\n",
      "Epoch 465/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 8.2354e-05\n",
      "Epoch 466/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 8.2354e-05\n",
      "Epoch 467/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 8.2354e-05\n",
      "Epoch 468/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 77ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 8.2354e-05\n",
      "Epoch 469/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 8.2354e-05\n",
      "Epoch 470/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 8.2354e-05\n",
      "Epoch 471/500\n",
      "\u001b[1m111/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0012\n",
      "Epoch 471: ReduceLROnPlateau reducing learning rate to 5.76480058953166e-05.\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 8.2354e-05\n",
      "Epoch 472/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - loss: 0.0013 - val_loss: 0.0013 - learning_rate: 5.7648e-05\n",
      "Epoch 473/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.7648e-05\n",
      "Epoch 474/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.7648e-05\n",
      "Epoch 475/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.7648e-05\n",
      "Epoch 476/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.7648e-05\n",
      "Epoch 477/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.7648e-05\n",
      "Epoch 478/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.7648e-05\n",
      "Epoch 479/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.7648e-05\n",
      "Epoch 480/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.7648e-05\n",
      "Epoch 481/500\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - loss: 0.0012 - val_loss: 0.0013 - learning_rate: 5.7648e-05\n",
      "Epoch 482/500\n",
      "\u001b[1m 93/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 0.0012"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# --------------------------\n",
    "# 1. 데이터 로드\n",
    "# --------------------------\n",
    "df_fault = pd.read_csv('dataset/outlier_data.csv')\n",
    "df_nor = pd.read_csv('dataset/press_data_normal.csv')\n",
    "\n",
    "use_col = ['AI0_Vibration', 'AI1_Vibration', 'AI2_Current']\n",
    "\n",
    "# 절댓값 처리\n",
    "df_nor[use_col] = df_nor[use_col].abs()\n",
    "df_fault[use_col] = df_fault[use_col].abs()\n",
    "\n",
    "# Feature / Label\n",
    "X_normal = df_nor[use_col]\n",
    "y_normal = df_nor['Equipment_state']\n",
    "X_anomaly = df_fault[use_col]\n",
    "y_anomaly = df_fault['Equipment_state']\n",
    "\n",
    "# --------------------------\n",
    "# 2. 데이터 합치기\n",
    "# --------------------------\n",
    "X_all = pd.concat([X_normal, X_anomaly], axis=0).reset_index(drop=True)\n",
    "y_all = pd.concat([y_normal, y_anomaly], axis=0).reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# 3. Train/Validation/Test 분리\n",
    "# --------------------------\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.15, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.15/0.85, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 4. MinMaxScaler\n",
    "# --------------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --------------------------\n",
    "# 5. 시퀀스 생성 함수\n",
    "# --------------------------\n",
    "def make_sequences(X, y, sequence=20, offset=100):\n",
    "    X_seq, Y_seq = [], []\n",
    "    max_index = len(X) - sequence - offset\n",
    "    if max_index <= 0:\n",
    "        return np.empty((0, sequence, X.shape[1])), np.empty((0,))\n",
    "    for i in range(max_index):\n",
    "        X_seq.append(X[i:i+sequence])\n",
    "        Y_seq.append(y[i + sequence + offset])\n",
    "    return np.array(X_seq), np.array(Y_seq)\n",
    "\n",
    "sequence = 20\n",
    "offset = 100\n",
    "\n",
    "X_train_seq, Y_train_seq = make_sequences(X_train_scaled, np.array(y_train), sequence, offset)\n",
    "X_valid_seq, Y_valid_seq = make_sequences(X_valid_scaled, np.array(y_valid), sequence, offset)\n",
    "X_test_seq, Y_test_seq = make_sequences(X_test_scaled, np.array(y_test), sequence, offset)\n",
    "\n",
    "# --------------------------\n",
    "# 6. 데이터 shape 확인\n",
    "# --------------------------\n",
    "print('X_train:', X_train_seq.shape, 'Y_train:', Y_train_seq.shape)\n",
    "print('X_valid:', X_valid_seq.shape, 'Y_valid:', Y_valid_seq.shape)\n",
    "print('X_test:', X_test_seq.shape, 'Y_test:', Y_test_seq.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 7. LSTM Autoencoder 정의\n",
    "# --------------------------\n",
    "def LSTM_AE(sequence, n_features):\n",
    "    lstm_ae = models.Sequential()\n",
    "    # 인코더\n",
    "    lstm_ae.add(layers.LSTM(64, input_shape=(sequence, n_features), return_sequences=True))\n",
    "    lstm_ae.add(layers.LSTM(32, return_sequences=False))\n",
    "    lstm_ae.add(layers.RepeatVector(sequence))\n",
    "    # 디코더\n",
    "    lstm_ae.add(layers.LSTM(32, return_sequences=True))\n",
    "    lstm_ae.add(layers.LSTM(64, return_sequences=True))\n",
    "    lstm_ae.add(layers.TimeDistributed(layers.Dense(n_features)))\n",
    "    return lstm_ae\n",
    "\n",
    "lstm_ae = LSTM_AE(sequence, X_train_seq.shape[2])\n",
    "lstm_ae.compile(loss='mse', optimizer=optimizers.Adam(0.001))\n",
    "\n",
    "# --------------------------\n",
    "# 8. Callback 정의\n",
    "# --------------------------\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=20, verbose=1)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=20, verbose=1,\n",
    "                   mode='min', restore_best_weights=True)\n",
    "\n",
    "# --------------------------\n",
    "# 9. 학습\n",
    "# --------------------------\n",
    "# Autoencoder 학습: 입력=출력\n",
    "history = lstm_ae.fit(\n",
    "    X_train_seq, X_train_seq,\n",
    "    epochs=500,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_valid_seq, X_valid_seq),\n",
    "    callbacks=[reduce_lr, es]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
